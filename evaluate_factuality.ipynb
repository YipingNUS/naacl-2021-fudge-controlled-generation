{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "from argparse import ArgumentParser\n",
    "from collections import namedtuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "\n",
    "from data import Dataset\n",
    "from model import Model\n",
    "from util import save_checkpoint, ProgressMeter, AverageMeter, num_params\n",
    "from constants import *\n",
    "from predict_factuality import predict_factuality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both models are bert-base-uncased and share the same tokenizer\n",
    "model_string = 'patrickvonplaten/bert2bert_cnn_daily_mail'\n",
    "attribute_model_string = 'textattack/bert-base-uncased-MNLI'\n",
    "device = 'cuda'\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model: patrickvonplaten/bert2bert_cnn_daily_mail...\n",
      "Loading pre-trained conditioning model: textattack/bert-base-uncased-MNLI...\n",
      "model num params 247363386\n",
      "conditioning_model num params 109484547\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(model_string)\n",
    "print(f\"Loading pre-trained model: {model_string}...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_string, return_dict=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loading pre-trained conditioning model: {attribute_model_string}...\")\n",
    "conditioning_model = AutoModelForSequenceClassification.from_pretrained(attribute_model_string).to(device)\n",
    "conditioning_model.eval()\n",
    "if verbose:\n",
    "    #checkpoint = torch.load(args.ckpt, map_location=args.device)\n",
    "    #print(f\"=> loaded checkpoint '{args.ckpt}' (epoch {checkpoint['epoch']})\")\n",
    "    print(f\"model num params {num_params(model)}\")\n",
    "    print(f\"conditioning_model num params {num_params(conditioning_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "with open('factuality_data/dummy_input.txt', 'r', encoding='utf-8') as rf:\n",
    "    for line in rf:\n",
    "        inputs.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/notebooks/fudge/predict_factuality.py\u001b[0m(129)\u001b[0;36m_generate_no_beam_search\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    127 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    128 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 129 \u001b[0;31m            expanded_lengths = torch.LongTensor(\n",
      "\u001b[0m\u001b[0;32m    130 \u001b[0;31m                [[cur_len for _ in range(precondition_topk)] for _ in range(batch_size)]).to(scores.device)\n",
      "\u001b[0m\u001b[0;32m    131 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  tplus1_candidates.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200, 169])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  encoder_input_ids.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 167])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  tplus1_candidate[0][0].shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'tplus1_candidate' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  tplus1_candidates[0,0,:].shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([169])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  print(tplus1_candidates[0,0,:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  1996,  3578,  2003, 27234,  3620,  1006,  1015,  1010,  5757,\n",
      "         2509,  3027,  1007,  4206,  1010,  2055,  1996,  2168,  4578,  2004,\n",
      "         2019,  6282,  1011, 11676,  2311,  1010,  1998,  1996, 13747,  3252,\n",
      "         1999,  3000,  1012,  2049,  2918,  2003,  2675,  1010,  9854,  8732,\n",
      "         3620,  1006, 19151,  3027,  1007,  2006,  2169,  2217,  1012,  2076,\n",
      "         2049,  2810,  1010,  1996,  1041, 13355,  2884,  3578, 15602,  1996,\n",
      "         2899,  6104,  2000,  2468,  1996, 13747,  2158,  1011,  2081,  3252,\n",
      "         1999,  1996,  2088,  1010,  1037,  2516,  2009,  2218,  2005,  4601,\n",
      "         2086,  2127,  1996, 17714,  2311,  1999,  2047,  2259,  2103,  2001,\n",
      "         2736,  1999,  4479,  1012,  2009,  2001,  1996,  2034,  3252,  2000,\n",
      "         3362,  1037,  4578,  1997,  3998,  3620,  1012,  2349,  2000,  1996,\n",
      "         2804,  1997,  1037,  5062,  9682,  2012,  1996,  2327,  1997,  1996,\n",
      "         3578,  1999,  3890,  1010,  2009,  2003,  2085, 12283,  2084,  1996,\n",
      "        17714,  2311,  2011,  1019,  1012,  1016,  3620,  1006,  2459,  3027,\n",
      "         1007,  1012, 13343, 26288,  1010,  1996,  1041, 13355,  2884,  3578,\n",
      "         2003,  1996,  2117, 13747,  2489,  1011,  3061,  3252,  1999,  2605,\n",
      "         2044,  1996,  4971,  4887, 20596,  1012,   102,  1996,   102],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  print(tplus1_candidates[0,1,:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  1996,  3578,  2003, 27234,  3620,  1006,  1015,  1010,  5757,\n",
      "         2509,  3027,  1007,  4206,  1010,  2055,  1996,  2168,  4578,  2004,\n",
      "         2019,  6282,  1011, 11676,  2311,  1010,  1998,  1996, 13747,  3252,\n",
      "         1999,  3000,  1012,  2049,  2918,  2003,  2675,  1010,  9854,  8732,\n",
      "         3620,  1006, 19151,  3027,  1007,  2006,  2169,  2217,  1012,  2076,\n",
      "         2049,  2810,  1010,  1996,  1041, 13355,  2884,  3578, 15602,  1996,\n",
      "         2899,  6104,  2000,  2468,  1996, 13747,  2158,  1011,  2081,  3252,\n",
      "         1999,  1996,  2088,  1010,  1037,  2516,  2009,  2218,  2005,  4601,\n",
      "         2086,  2127,  1996, 17714,  2311,  1999,  2047,  2259,  2103,  2001,\n",
      "         2736,  1999,  4479,  1012,  2009,  2001,  1996,  2034,  3252,  2000,\n",
      "         3362,  1037,  4578,  1997,  3998,  3620,  1012,  2349,  2000,  1996,\n",
      "         2804,  1997,  1037,  5062,  9682,  2012,  1996,  2327,  1997,  1996,\n",
      "         3578,  1999,  3890,  1010,  2009,  2003,  2085, 12283,  2084,  1996,\n",
      "        17714,  2311,  2011,  1019,  1012,  1016,  3620,  1006,  2459,  3027,\n",
      "         1007,  1012, 13343, 26288,  1010,  1996,  1041, 13355,  2884,  3578,\n",
      "         2003,  1996,  2117, 13747,  2489,  1011,  3061,  3252,  1999,  2605,\n",
      "         2044,  1996,  4971,  4887, 20596,  1012,   102,  1041,   102],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for inp in tqdm(inputs, total=len(inputs)):\n",
    "    results = predict_factuality(model,\n",
    "                    tokenizer, \n",
    "                    conditioning_model, \n",
    "                    [inp],\n",
    "                    precondition_topk=200,\n",
    "                    do_sample=False,\n",
    "                    min_length=30,\n",
    "                    max_length=90,\n",
    "                    condition_lambda=1.0,\n",
    "                    device=device)\n",
    "    print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_0 = \"The company HuggingFace is based in New York City. Google is located in Mount Hill.\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in New York\"\n",
    "tokenized_data =tokenizer([sequence_0, sequence_2], [sequence_2, sequence_0], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([  101,  1996,  3578,  2003, 27234,  3620,  1006,  1015,  1010,  5757,\n",
    "         2509,  3027,  1007,  4206,  1010,  2055,  1996,  2168,  4578,  2004,\n",
    "         2019,  6282,  1011, 11676,  2311,  1010,  1998,  1996, 13747,  3252,\n",
    "         1999,  3000,  1012,  2049,  2918,  2003,  2675,  1010,  9854,  8732,\n",
    "         3620,  1006, 19151,  3027,  1007,  2006,  2169,  2217,  1012,  2076,\n",
    "         2049,  2810,  1010,  1996,  1041, 13355,  2884,  3578, 15602,  1996,\n",
    "         2899,  6104,  2000,  2468,  1996, 13747,  2158,  1011,  2081,  3252,\n",
    "         1999,  1996,  2088,  1010,  1037,  2516,  2009,  2218,  2005,  4601,\n",
    "         2086,  2127,  1996, 17714,  2311,  1999,  2047,  2259,  2103,  2001,\n",
    "         2736,  1999,  4479,  1012,  2009,  2001,  1996,  2034,  3252,  2000,\n",
    "         3362,  1037,  4578,  1997,  3998,  3620,  1012,  2349,  2000,  1996,\n",
    "         2804,  1997,  1037,  5062,  9682,  2012,  1996,  2327,  1997,  1996,\n",
    "         3578,  1999,  3890,  1010,  2009,  2003,  2085, 12283,  2084,  1996,\n",
    "        17714,  2311,  2011,  1019,  1012,  1016,  3620,  1006,  2459,  3027,\n",
    "         1007,  1012, 13343, 26288,  1010,  1996,  1041, 13355,  2884,  3578,\n",
    "         2003,  1996,  2117, 13747,  2489,  1011,  3061,  3252,  1999,  2605,\n",
    "         2044,  1996,  4971,  4887, 20596,  1012,   102,  1996,   102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
