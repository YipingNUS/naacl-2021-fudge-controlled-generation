{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "from argparse import ArgumentParser\n",
    "from collections import namedtuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "\n",
    "from data import Dataset\n",
    "from model import Model\n",
    "from util import save_checkpoint, ProgressMeter, AverageMeter, num_params\n",
    "from constants import *\n",
    "from predict_factuality import predict_factuality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both models are bert-base-uncased and share the same tokenizer\n",
    "model_string = 'patrickvonplaten/bert2bert_cnn_daily_mail'\n",
    "attribute_model_string = 'textattack/bert-base-uncased-MNLI'\n",
    "device = 'cuda'\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model: patrickvonplaten/bert2bert_cnn_daily_mail...\n",
      "Loading pre-trained conditioning model: textattack/bert-base-uncased-MNLI...\n",
      "model num params 247363386\n",
      "conditioning_model num params 109484547\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(model_string)\n",
    "print(f\"Loading pre-trained model: {model_string}...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_string, return_dict=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loading pre-trained conditioning model: {attribute_model_string}...\")\n",
    "conditioning_model = AutoModelForSequenceClassification.from_pretrained(attribute_model_string).to(device)\n",
    "conditioning_model.eval()\n",
    "if verbose:\n",
    "    #checkpoint = torch.load(args.ckpt, map_location=args.device)\n",
    "    #print(f\"=> loaded checkpoint '{args.ckpt}' (epoch {checkpoint['epoch']})\")\n",
    "    print(f\"model num params {num_params(model)}\")\n",
    "    print(f\"conditioning_model num params {num_params(conditioning_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "with open('factuality_data/dummy_input.txt', 'r', encoding='utf-8') as rf:\n",
    "    for line in rf:\n",
    "        inputs.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:59<00:00, 59.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tallest structure in paris is the eiffel tower, which was built by the french tallest building. it is now taller than the chrysler building and has been used since 1957.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for inp in tqdm(inputs, total=len(inputs)):\n",
    "    results = predict_factuality(model,\n",
    "                    tokenizer, \n",
    "                    conditioning_model, \n",
    "                    [inp],\n",
    "                    precondition_topk=200,\n",
    "                    do_sample=False,\n",
    "                    min_length=30,\n",
    "                    max_length=90,\n",
    "                    condition_lambda=0.5,\n",
    "                    device=device)\n",
    "    print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the real CNN-DM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602)\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "val_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation[:1%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': '(CNN)Ferguson is crumbling. The cowardly and reprehensible shooting Wednesday night of two police officers came in a tumultuous seven days for the Missouri town, which had already seen Ferguson Police Chief Thomas Jackson announce his resignation after a damning Justice Department report on its police department. The report, which was ordered in the wake of the killing of Michael Brown last year, highlighted a predatory policing problem and a department that was biased, prejudiced and that has regularly targeted, arrested and fined African-Americans. Residents understandably want justice. But what\\'s worse in all this is that Ferguson is illustrative of a broader problem across the country as increasingly militarized majority-white police departments demonstrate consistent racial bias toward majority-black communities. It\\'s a combustible mix. In three-quarters of all U.S. cities with populations 50,000 or more, the police presence is \"disproportionately white relative to the local population,\" according to The Washington Post. And tensions are being exacerbated by the use among police departments of military weapons, and stipulations that these former war zone weapons must be used within a year of acquisition. All this suggests a need for a completely new mindset on how we try to understand and implement policing practices across America.  Indeed, a wholesale review of policies and approaches to law enforcement is needed, something that will likely necessitate drastic reforms in some departments so they can better represent, integrate, problem solve and liaise with the communities they are serving. First and foremost, our police departments must better reflect the diverse demographics of our increasingly diverse nation, whether it be race, creed, sexual orientation and more. America is changing fast, but police departments aren\\'t keeping up. Training and recruitment of minorities is critical, yet far more needs to be taking place. With this in mind, amplifying community policing models that work and scaling them up immediately is essential if we are to stem the growing and sometimes overwhelming tide of frustration, anger and cynicism welling up among young African, Asian and Hispanic Americans. Second, we must radically rethink the trend toward the indiscriminate procurement and use of surplus military grade weaponry, which under the Department of Defense\\'s 1033 program is flowing from battlefields to our local police forces.  When the weapons of war come home from Iraq and Afghanistan to help police America\\'s cities and towns, then you know something has gone terribly wrong with this country. Mine-resistant ambush protected vehicles, tanks, drones, grenades and assault weapons should not replace the community policing of our Main Streets. Ferguson is an excellent example of how the deployment of heavy weaponry inflames rather than de-escalates a crisis situation. Having military equipment on our streets does not make citizens feel safer, which is why the Stop Militarizing Law Enforcement Act was recently reintroduced in Congress. We do not need our officers looking like \"Robocop\" when they patrol our streets. It\\'s that simple. Yet until such a bill is passed, war weapons will continue to flood our streets; Congress must act to stem this tide. We understand that times have changed and that new security threats require new solutions and procedures.  But it doesn\\'t justify the 150 raids per day by special weapons and tactics units for incidents that can be as benign as a Department of Education warrant. This kind of aggressive approach doesn\\'t engender the kind of engagement necessary for identifying real risks lurking in a community.  In fact, the opposite happens.  Intelligence opportunities are dead on arrival, and potential allies who would otherwise be ready to help shut down immediately. Aggressive military-type action is quickly turning Americans against fellow citizens who they are ostensibly there to serve and protect. We therefore trust that President Barack Obama and Attorney General Eric Holder will take decisive action, working hand in hand with police departments all across this country. Yes, the White House\\'s task force on police militarization was a start, but more concrete measures are needed if we want to reverse the rising anger in Ferguson and elsewhere. The time for a change is now. If we don\\'t press our police departments to reflect the makeup and needs of our communities, then towns like Ferguson will unravel further.',\n",
       " 'highlights': 'Two police officers were shot Wednesday in Ferguson .\\nHank Johnson, Michael Shank: Policing style needs rethink .',\n",
       " 'id': '00c45eb98a06f9218170edf5767617cc20991840'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = val_data[3]\n",
    "inp = str(example['article'])\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict_factuality(model,\n",
    "                    tokenizer, \n",
    "                    conditioning_model, \n",
    "                    [inp],\n",
    "                    precondition_topk=200,\n",
    "                    do_sample=False,\n",
    "                    min_length=30,\n",
    "                    max_length=90,\n",
    "                    condition_lambda=0.0,\n",
    "                    device=device)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = predict_factuality(model,\n",
    "                    tokenizer, \n",
    "                    conditioning_model, \n",
    "                    [inp],\n",
    "                    precondition_topk=200,\n",
    "                    do_sample=False,\n",
    "                    min_length=30,\n",
    "                    max_length=90,\n",
    "                    condition_lambda=1.0,\n",
    "                    device=device)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch prediction and evaluate with ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d27b9b8a424493b2044104d4229f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=134.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tnrange\n",
    "ground_truths, baseline_preds = list(), list()\n",
    "for i in tnrange(len(val_data)):\n",
    "    example = val_data[i]\n",
    "    ground_truths.append(example['highlights'])\n",
    "    inp = str(example['article'])\n",
    "    pred = predict_factuality(model,\n",
    "                    tokenizer, \n",
    "                    conditioning_model, \n",
    "                    [inp],\n",
    "                    precondition_topk=200,\n",
    "                    do_sample=False,\n",
    "                    min_length=30,\n",
    "                    max_length=90,\n",
    "                    condition_lambda=0.0,\n",
    "                    device=device)[0]\n",
    "    baseline_preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.2973886056202959, recall=0.29614087733837935, fmeasure=0.2884923559900645), mid=Score(precision=0.32652823393269714, recall=0.32132599812241225, fmeasure=0.31466521619878635), high=Score(precision=0.3562696664088422, recall=0.3496707382447767, fmeasure=0.34034349993756213)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.10140779724122802, recall=0.10000367038351103, fmeasure=0.09795803188875224), mid=Score(precision=0.12738807145162467, recall=0.12617802620784818, fmeasure=0.1239448374378738), high=Score(precision=0.15785803937875417, recall=0.1566275179534468, fmeasure=0.15346734024983144)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.21395823140953238, recall=0.21507604393472093, fmeasure=0.20929645243874456), mid=Score(precision=0.23793875573848006, recall=0.23952351909790798, fmeasure=0.23212024089348565), high=Score(precision=0.26698111309424244, recall=0.2678733453458605, fmeasure=0.2600093795293905))}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=baseline_preds, references=ground_truths, rouge_types=['rouge1', 'rouge2', 'rougeL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
